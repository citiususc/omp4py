import token
import tokenize
import dataclasses
from io import StringIO
from typing import Iterator

from token import *

__all__ = ['TokenInfo', 'generate_tokens', 'expected_error', 'untokenize', 'merge',
           'indent_size', 'str_multiline'] + token.__all__


@dataclasses.dataclass(frozen=True)
class TokenInfo:
    """
    Represents information about a token in directives and clauses.

    Attributes:
        filename (str): The name of the file where the token is located.
        lineno (int): The line number in the file where the token is located.
        type (int): The type of the token (e.g., NAME, COMMA, NUMBER).
        string (str): The actual string representing the token (e.g., 'if', 'x', '+').
        start (tuple[int, int]): The (row, column) position of the start of the token in the input.
        end (tuple[int, int]): The (row, column) position of the end of the token in the input.
        line (str): The full line of code in which the token is found.
    """
    filename: str
    lineno: int
    type: int
    string: str
    start: tuple[int, int]
    end: tuple[int, int]
    line: str

    def __str__(self) -> str:
        """Returns the string representation of the token"""
        return self.string

    @property
    def id(self) -> str:
        """Returns the string representation of the token in lowercase"""
        return self.string.lower()

    def make_error(self, msg: str) -> SyntaxError:
        """Generates a SyntaxError with detailed information about the token's position and the error message.

        Args:
            msg (str): The error message to be included in the SyntaxError.

        Returns:
            SyntaxError: A SyntaxError instance with detailed location information.
        """
        return SyntaxError(
            msg,
            (
                self.filename,  # File where the error occurred.
                self.start[0] + self.lineno - 1,  # Line number of the token (accounting for any previous lines).
                self.start[1] + 1,  # Column number of the token (start position).
                self.line,  # The full line of code where the token is located.
                self.end[0] + self.lineno - 1,
                # End line number of the token (accounting for any previous lines).
                self.end[1] + 1  # End column number of the token.
            )
        )


def expected_error(t: TokenInfo, expected: str) -> SyntaxError:
    """
    Generates a SyntaxError indicating that the expected token is missing or incorrect.

    Args:
        t (TokenInfo): The TokenInfo object representing the encountered token.
        expected (str): A string describing the expected token or type.

    Returns:
        SyntaxError: An error generated by the `make_error` method of the TokenInfo object.
    """
    if t.type == tokenize.ENDMARKER:
        before_msg = "end of line"
    elif t.type == tokenize.NAME:
        before_msg = f"'{t.string}'"
    else:
        before_msg = f"'{t.string}' token"
    return t.make_error(f"expected {expected} before {before_msg}")


def eof_token(last: TokenInfo) -> TokenInfo:
    """
    Generates a TokenInfo object representing the end of file (EOF) token.

    Args:
        last (TokenInfo): The last token processed, which provides the information about the last token's location.

    Returns:
        TokenInfo: A TokenInfo object representing the EOF token.
    """
    return TokenInfo(
        filename=last.filename,
        lineno=last.lineno,
        type=tokenize.ENDMARKER,
        string="",
        start=last.end,
        end=(last.end[0] + 1, last.end[1]),
        line=last.line + " "
    )


def indent_size(lines: list[str]) -> int:
    lexer: Iterator[tokenize.TokenInfo] = tokenize.generate_tokens(StringIO(''.join(lines)).readline)
    token: tokenize.TokenInfo
    for token in lexer:
        if token.type == tokenize.INDENT:
            return len(token.string)
    return -1


def preproc_line(line: str) -> str:
    line = f"(\n{line}\n)"
    lexer: Iterator[tokenize.TokenInfo] = tokenize.generate_tokens(StringIO(line).readline)
    tokens: list[tokenize.TokenInfo] = list(filter(lambda t: t.type == STRING, lexer))

    result: str = ""
    lineno: int = 2
    offset: int = 0
    token: tokenize.TokenInfo
    for token in tokens:
        while lineno < token.start[0]:
            result += "\n"
            lineno += 1
            offset = 0
        sep: int = 3 if token.string.startswith('"""') or token.string.startswith("'''") else 1
        padding: int = (token.start[1] - offset + sep)
        result += ' ' * padding + token.string[sep:-sep]
        offset += padding + len(token.string[sep:-sep])

    return f"(\n{result}\n)"


def generate_tokens(filename: str, line: str, lineno: int, preproc=False) -> (list[TokenInfo], str):
    """
    Generates tokens from a line of code and provides information about any syntax errors encountered.

    Args:
        filename (str): The name of the file where the line is located.
        line (str): The line of code to parse, containing the OpenMP directives.
        lineno (int): The line number in the file where the input is located.
        preproc (bool, optional): If True, preprocesses the line before parsing.

    Returns:
        tuple: A tuple containing:
            - `tokens` (list[TokenInfo]): A list of `TokenInfo` objects representing the tokens found in the line.
            - `error` (str): An error message if any syntax error occurred during tokenization, or `None` if no error
                             occurred.

    Raises:
        SyntaxError: If a malformed line is encountered (excluding "EOF" errors).
    """
    proc_line: str = preproc_line(line) if preproc else line

    lexer: Iterator[tokenize.TokenInfo] = tokenize.generate_tokens(StringIO(proc_line).readline)
    tokens: list[TokenInfo] = []
    error: str | None = None

    try:
        for next_token in lexer:
            if len(next_token.string.strip()) > 0:
                tokens.append(TokenInfo(filename, lineno, *next_token._replace(type=next_token.exact_type)))
                if preproc:
                    tokens[-1] = dataclasses.replace(tokens[-1],
                                                     start=(tokens[-1].start[0] - 1, tokens[-1].start[1]),
                                                     end=(tokens[-1].end[0] - 1, tokens[-1].end[1]))
    except tokenize.TokenError as ex:
        error = str(ex)
        # If the error is related to EOF, allow the parser to generate a more informative error message.
        # This gives the parser a chance to provide better context.
        if "EOF" not in error:
            raise tokens[-1].make_error("malformed line after token")
    if preproc:
        if error is None:
            tokens = tokens[1:-1]
        else:
            tokens = tokens[1:]

    tokens.append(eof_token(tokens[-1]))

    return tokens, error


def untokenize(tokens: list[TokenInfo] | tuple[TokenInfo, ...]) -> str:
    """
    Converts a list of `TokenInfo` objects back into a string of code.

    Args:
        tokens (list[TokenInfo]): A list of `TokenInfo` objects representing the tokens to be untokenized.

    Returns:
        str: The reconstructed line of code as a string.
    """
    return tokenize.untokenize([tokenize.TokenInfo(**{key: value for key, value in t.__dict__.items()
                                                      if key in tokenize.TokenInfo._fields}) for t in tokens])


def merge(tokens: list[TokenInfo] | tuple[TokenInfo, ...]) -> TokenInfo:
    """
    Merges a list of TokenInfo objects into a single TokenInfo object.

    Args:
        tokens (list[TokenInfo]): A list of TokenInfo objects to be merged.

    Returns:
        TokenInfo: A new TokenInfo object that represents the merged tokens, with an updated
                   string (created by untokenizing the list of tokens).
    """
    if len(tokens) == 1:
        return tokens[0]

    return dataclasses.replace(tokens[0], string=untokenize(tokens), end=tokens[-1].end)
